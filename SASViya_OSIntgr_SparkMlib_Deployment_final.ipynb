{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Source Analytical Lifecycle : Deploying an Mlib model on Hadoop Via SAS Model Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Purpose: Show an end-to-end analytical lifecycle for Python Model via SAS Model Manager*\n",
    "\n",
    "*Author: Ivan Nardini (EDM Customer Advisor)*\n",
    "\n",
    "*Date (Last Update): Sept, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Enviroment-checks-and-settings\" data-toc-modified-id=\"Enviroment-checks-and-settings-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Enviroment checks and settings</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#Model-Development-with-Spark-MLib\" data-toc-modified-id=\"Model-Development-with-Spark-MLib-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model Development with Spark MLib</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-regression-model\" data-toc-modified-id=\"Logistic-regression-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Logistic regression model</a></span></li><li><span><a href=\"#Evaluate-the-model\" data-toc-modified-id=\"Evaluate-the-model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Evaluate the model</a></span></li><li><span><a href=\"#Comment\" data-toc-modified-id=\"Comment-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Comment</a></span></li></ul></li><li><span><a href=\"#Model-Deployment-with-SAS-Model-Manager-and-SAS-Workflow-Manager\" data-toc-modified-id=\"Model-Deployment-with-SAS-Model-Manager-and-SAS-Workflow-Manager-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model Deployment with SAS Model Manager and SAS Workflow Manager</a></span><ul class=\"toc-item\"><li><span><a href=\"#Testing-the-scoring-code\" data-toc-modified-id=\"Testing-the-scoring-code-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Testing the scoring code</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-the-model\" data-toc-modified-id=\"Read-the-model-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Read the model</a></span></li><li><span><a href=\"#Load-new-data\" data-toc-modified-id=\"Load-new-data-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Load new data</a></span></li><li><span><a href=\"#Score-new-data\" data-toc-modified-id=\"Score-new-data-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Score new data</a></span></li></ul></li><li><span><a href=\"#.py-with-the-scoring-code-(scoring-model)\" data-toc-modified-id=\".py-with-the-scoring-code-(scoring-model)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>.py with the scoring code (scoring model)</a></span></li></ul></li><li><span><a href=\"#Register-the-model-in-SAS-Model-Manager-via-REST-API-call\" data-toc-modified-id=\"Register-the-model-in-SAS-Model-Manager-via-REST-API-call-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Register the model in SAS Model Manager via REST API call</a></span><ul class=\"toc-item\"><li><span><a href=\"#Zip-Spark-Mlib-model-score-code\" data-toc-modified-id=\"Zip-Spark-Mlib-model-score-code-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Zip Spark Mlib model score code</a></span></li><li><span><a href=\"#Zip-Spark-Mlib-Model-files\" data-toc-modified-id=\"Zip-Spark-Mlib-Model-files-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Zip Spark Mlib Model files</a></span></li><li><span><a href=\"#Get-the-token\" data-toc-modified-id=\"Get-the-token-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Get the token</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-Model-Manager-repository\" data-toc-modified-id=\"Create-a-Model-Manager-repository-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Create a Model Manager repository</a></span></li><li><span><a href=\"#Create-a-Model-Manager-Folder\" data-toc-modified-id=\"Create-a-Model-Manager-Folder-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Create a Model Manager Folder</a></span></li><li><span><a href=\"#Create-a-New-Model-Manager-Project\" data-toc-modified-id=\"Create-a-New-Model-Manager-Project-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Create a New Model Manager Project</a></span></li><li><span><a href=\"#Register-the-score-code-to-SAS-Model-Manager\" data-toc-modified-id=\"Register-the-score-code-to-SAS-Model-Manager-5.3.4\"><span class=\"toc-item-num\">5.3.4&nbsp;&nbsp;</span>Register the score code to SAS Model Manager</a></span></li><li><span><a href=\"#Register-the-Spark-Model-files-to-SAS-Model-Manager\" data-toc-modified-id=\"Register-the-Spark-Model-files-to-SAS-Model-Manager-5.3.5\"><span class=\"toc-item-num\">5.3.5&nbsp;&nbsp;</span>Register the Spark Model files to SAS Model Manager</a></span></li></ul></li></ul></li><li><span><a href=\"#Python-codes-for-SAS-Workflow-Manager\" data-toc-modified-id=\"Python-codes-for-SAS-Workflow-Manager-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Python codes for SAS Workflow Manager</a></span><ul class=\"toc-item\"><li><span><a href=\"#.py-for-scoring\" data-toc-modified-id=\".py-for-scoring-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>.py for scoring</a></span></li><li><span><a href=\"#.py-for-retrain-model\" data-toc-modified-id=\".py-for-retrain-model-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>.py for retrain model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroment checks and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T14:47:35.417664Z",
     "start_time": "2019-10-02T14:47:35.413654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.7 (default, Jul  2 2019, 02:21:41) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T14:47:41.188498Z",
     "start_time": "2019-10-02T14:47:40.445991Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import requests as req\n",
    "import json\n",
    "import random\n",
    "import pprint as pp\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import zipfile\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# findspark.find()\n",
    "# import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:08:55.114818Z",
     "start_time": "2019-09-24T13:08:55.105283Z"
    }
   },
   "outputs": [],
   "source": [
    "#Zip function\n",
    "\n",
    "def zipdir(src, dst): #take source code file and choose a destination\n",
    "    zf = zipfile.ZipFile('%s' % (dst), \"w\", zipfile.ZIP_DEFLATED) #Open zip file\n",
    "    abs_src = os.path.abspath(src) \n",
    "    for dirname, subdirs, files in os.walk(src): #generate the directory tree\n",
    "        for filename in files:\n",
    "            absname = os.path.abspath(os.path.join(dirname, filename)) \n",
    "            arcname = absname[len(abs_src) + 1:]\n",
    "            print (\"zipping %s as %s\" % (os.path.join(dirname, filename),arcname))\n",
    "            zf.write(absname, arcname),\n",
    "    zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T14:48:23.852075Z",
     "start_time": "2019-10-02T14:48:23.849067Z"
    }
   },
   "outputs": [],
   "source": [
    "#Global variables \n",
    "\n",
    "outpath=os.getcwd()\n",
    "randid=random.randint(1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:09:05.449549Z",
     "start_time": "2019-09-24T13:09:05.399912Z"
    }
   },
   "outputs": [],
   "source": [
    "indata     = 'hmeq.csv'\n",
    "df_data_card = pd.read_csv(indata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:09:06.009573Z",
     "start_time": "2019-09-24T13:09:05.948411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAD</th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>REASON</th>\n",
       "      <th>JOB</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>DEROG</th>\n",
       "      <th>DELINQ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1100</td>\n",
       "      <td>25860.0</td>\n",
       "      <td>39025.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.366667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1300</td>\n",
       "      <td>70053.0</td>\n",
       "      <td>68400.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>16700.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1700</td>\n",
       "      <td>97800.0</td>\n",
       "      <td>112000.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Office</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BAD  LOAN  MORTDUE     VALUE   REASON     JOB   YOJ  DEROG  DELINQ  \\\n",
       "0    1  1100  25860.0   39025.0  HomeImp   Other  10.5    0.0     0.0   \n",
       "1    1  1300  70053.0   68400.0  HomeImp   Other   7.0    0.0     2.0   \n",
       "2    1  1500  13500.0   16700.0  HomeImp   Other   4.0    0.0     0.0   \n",
       "3    1  1500      NaN       NaN      NaN     NaN   NaN    NaN     NaN   \n",
       "4    0  1700  97800.0  112000.0  HomeImp  Office   3.0    0.0     0.0   \n",
       "\n",
       "        CLAGE  NINQ  CLNO  DEBTINC  \n",
       "0   94.366667   1.0   9.0      NaN  \n",
       "1  121.833333   0.0  14.0      NaN  \n",
       "2  149.466667   1.0  10.0      NaN  \n",
       "3         NaN   NaN   NaN      NaN  \n",
       "4   93.333333   0.0  14.0      NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:09:06.522242Z",
     "start_time": "2019-09-24T13:09:06.495165Z"
    }
   },
   "outputs": [],
   "source": [
    "target = df_data_card.columns[0]\n",
    "class_inputs = list(df_data_card.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:09:07.071884Z",
     "start_time": "2019-09-24T13:09:06.979630Z"
    }
   },
   "outputs": [],
   "source": [
    "# impute data\n",
    "df_data_card = df_data_card.fillna(df_data_card.median())\n",
    "df_data_card['JOB'] = df_data_card.JOB.fillna('Other')\n",
    "\n",
    "# dummy the categorical variables\n",
    "df_data_card_ABT = pd.concat([df_data_card, pd.get_dummies(df_data_card[class_inputs])], axis = 1).drop(class_inputs, axis = 1)\n",
    "df_all_inputs = df_data_card_ABT.drop(target, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:09:07.566248Z",
     "start_time": "2019-09-24T13:09:07.502065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>DEROG</th>\n",
       "      <th>DELINQ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "      <th>REASON_DebtCon</th>\n",
       "      <th>REASON_HomeImp</th>\n",
       "      <th>JOB_Mgr</th>\n",
       "      <th>JOB_Office</th>\n",
       "      <th>JOB_Other</th>\n",
       "      <th>JOB_ProfExe</th>\n",
       "      <th>JOB_Sales</th>\n",
       "      <th>JOB_Self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100</td>\n",
       "      <td>25860.0</td>\n",
       "      <td>39025.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.366667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>34.818262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1300</td>\n",
       "      <td>70053.0</td>\n",
       "      <td>68400.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.818262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>16700.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.818262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500</td>\n",
       "      <td>65019.0</td>\n",
       "      <td>89235.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.818262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1700</td>\n",
       "      <td>97800.0</td>\n",
       "      <td>112000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.818262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LOAN  MORTDUE     VALUE   YOJ  DEROG  DELINQ       CLAGE  NINQ  CLNO  \\\n",
       "0  1100  25860.0   39025.0  10.5    0.0     0.0   94.366667   1.0   9.0   \n",
       "1  1300  70053.0   68400.0   7.0    0.0     2.0  121.833333   0.0  14.0   \n",
       "2  1500  13500.0   16700.0   4.0    0.0     0.0  149.466667   1.0  10.0   \n",
       "3  1500  65019.0   89235.5   7.0    0.0     0.0  173.466667   1.0  20.0   \n",
       "4  1700  97800.0  112000.0   3.0    0.0     0.0   93.333333   0.0  14.0   \n",
       "\n",
       "     DEBTINC  REASON_DebtCon  REASON_HomeImp  JOB_Mgr  JOB_Office  JOB_Other  \\\n",
       "0  34.818262               0               1        0           0          1   \n",
       "1  34.818262               0               1        0           0          1   \n",
       "2  34.818262               0               1        0           0          1   \n",
       "3  34.818262               0               0        0           0          1   \n",
       "4  34.818262               0               1        0           1          0   \n",
       "\n",
       "   JOB_ProfExe  JOB_Sales  JOB_Self  \n",
       "0            0          0         0  \n",
       "1            0          0         0  \n",
       "2            0          0         0  \n",
       "3            0          0         0  \n",
       "4            0          0         0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:09:07.981910Z",
     "start_time": "2019-09-24T13:09:07.977899Z"
    }
   },
   "outputs": [],
   "source": [
    "abt=pd.concat([df_all_inputs, df_data_card[target]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:09:08.770469Z",
     "start_time": "2019-09-24T13:09:08.580947Z"
    }
   },
   "outputs": [],
   "source": [
    "abt.to_csv(outpath+'/datatoscore.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development with Spark MLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:10:31.929666Z",
     "start_time": "2019-09-24T13:10:31.924653Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:10:33.657761Z",
     "start_time": "2019-09-24T13:10:32.477122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LOAN: integer (nullable = true)\n",
      " |-- MORTDUE: double (nullable = true)\n",
      " |-- VALUE: double (nullable = true)\n",
      " |-- YOJ: double (nullable = true)\n",
      " |-- DEROG: double (nullable = true)\n",
      " |-- DELINQ: double (nullable = true)\n",
      " |-- CLAGE: double (nullable = true)\n",
      " |-- NINQ: double (nullable = true)\n",
      " |-- CLNO: double (nullable = true)\n",
      " |-- DEBTINC: double (nullable = true)\n",
      " |-- REASON_DebtCon: integer (nullable = true)\n",
      " |-- REASON_HomeImp: integer (nullable = true)\n",
      " |-- JOB_Mgr: integer (nullable = true)\n",
      " |-- JOB_Office: integer (nullable = true)\n",
      " |-- JOB_Other: integer (nullable = true)\n",
      " |-- JOB_ProfExe: integer (nullable = true)\n",
      " |-- JOB_Sales: integer (nullable = true)\n",
      " |-- JOB_Self: integer (nullable = true)\n",
      " |-- BAD: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('ml-creditscore').getOrCreate()\n",
    "df = spark.read.csv('datatoscore.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:10:37.564850Z",
     "start_time": "2019-09-24T13:10:37.064305Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assemble features into one vector for the model\n",
    "ignore = ['BAD']\n",
    "assembler = VectorAssembler(inputCols=[x for x in df.columns if x not in ignore],\n",
    "                            outputCol='features')\n",
    "df1 = assembler.transform(df)\n",
    "dataset = df1.select(col('features'),col('BAD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:10:41.798626Z",
     "start_time": "2019-09-24T13:10:39.952204Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 4679\n",
      "Test Dataset Count: 1281\n"
     ]
    }
   ],
   "source": [
    "train, test = dataset.randomSplit([0.8, 0.2], seed=83)\n",
    "\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:10:49.792403Z",
     "start_time": "2019-09-24T13:10:47.030298Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol = 'BAD', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:10:50.684609Z",
     "start_time": "2019-09-24T13:10:50.222332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+--------------------+\n",
      "|BAD|       rawPrediction|prediction|         probability|\n",
      "+---+--------------------+----------+--------------------+\n",
      "|  0|[1.39505276550396...|       0.0|[0.80139766337662...|\n",
      "|  1|[-2.3642017106207...|       1.0|[0.08594354489623...|\n",
      "|  0|[1.83473264535823...|       0.0|[0.86232455488486...|\n",
      "|  0|[1.87326738067525...|       0.0|[0.86683588815967...|\n",
      "|  1|[1.35736811762912...|       0.0|[0.79533161489755...|\n",
      "|  1|[1.54584667135224...|       0.0|[0.82431305241083...|\n",
      "|  0|[2.23084013344774...|       0.0|[0.90298498223301...|\n",
      "|  0|[2.54991322755416...|       0.0|[0.92756768497209...|\n",
      "|  1|[1.86472511861982...|       0.0|[0.86584674906562...|\n",
      "|  0|[0.25353354355120...|       0.0|[0.56304603444238...|\n",
      "+---+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.select('BAD', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:27:41.707874Z",
     "start_time": "2019-09-04T13:27:41.035569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.7870646246757518\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='BAD')\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:27:42.043267Z",
     "start_time": "2019-09-04T13:27:42.032236Z"
    }
   },
   "outputs": [],
   "source": [
    "def evalMetrics(actual, predicted, dataframe):\n",
    "    '''\n",
    "    Calculates evaluation metrics from predicted results\n",
    "    \n",
    "    Input:\n",
    "    ---------\n",
    "        actual: Column of the DataFrame with the real values [string]\n",
    "        predicted: Column of the DataFrame with the predicted values [string]\n",
    "        dataframe: spark.sql.dataframe with the real and predicted values [object]\n",
    "    \n",
    "    Output:\n",
    "    ---------\n",
    "        spark.sql.dataframe with evaluation metrics.\n",
    "    '''\n",
    "       \n",
    "    # Along each row are the actual values and down each column are the predicted\n",
    "    dataframe = dataframe.withColumn(actual, col(actual).cast('integer'))\n",
    "    dataframe = dataframe.withColumn(predicted, col(predicted).cast('integer'))\n",
    "    cm = dataframe.crosstab(actual, predicted)\n",
    "    cm = cm.sort(cm.columns[0], ascending = True)\n",
    "    \n",
    "    # Adds missing column in case just one class was predicted\n",
    "    if not '0' in cm.columns:\n",
    "        cm = cm.withColumn('0', lit(0))\n",
    "    if not '1' in cm.columns:\n",
    "        cm = cm.withColumn('1', lit(0))\n",
    "    \n",
    "    # Subsets values from confussion matrix\n",
    "    zero = cm.filter(cm[cm.columns[0]] == 0.0)\n",
    "    first_0 = zero.take(1)\n",
    "    \n",
    "    one = cm.filter(cm[cm.columns[0]] == 1.0)\n",
    "    first_1 = one.take(1)\n",
    "    \n",
    "    tn = first_0[0][1]\n",
    "    fp = first_0[0][2]\n",
    "    fn = first_1[0][1]\n",
    "    tp = first_1[0][2]\n",
    "    \n",
    "    # Calculate metrics from values in the confussion matrix\n",
    "    if (tp == 0):\n",
    "        acc = float((tp + tn) / (tp + tn + fp + fn))\n",
    "        sen = 0\n",
    "        spe = float((tn) / (tn + fp))\n",
    "        prec = 0\n",
    "        rec = 0\n",
    "        f1 = 0\n",
    "    elif (tn == 0):\n",
    "        acc = float((tp + tn) / (tp + tn + fp + fn))\n",
    "        sen = float((tp) / (tp + fn))\n",
    "        spe = 0\n",
    "        prec = float((tp) / (tp + fp))\n",
    "        rec = float((tp) / (tp + fn))\n",
    "        f1 = 2 * float((prec * rec) / (prec + rec))\n",
    "    else:\n",
    "        acc = float((tp + tn) / (tp + tn + fp + fn))\n",
    "        sen = float((tp) / (tp + fn))\n",
    "        spe = float((tn) / (tn + fp))\n",
    "        prec = float((tp) / (tp + fp))\n",
    "        rec = float((tp) / (tp + fn))\n",
    "        f1 = 2 * float((prec * rec) / (prec + rec))\n",
    "\n",
    "    # Print results\n",
    "    print('Confusion Matrix and Statistics: \\n')\n",
    "    cm.show()\n",
    "    \n",
    "    print('True Positives:', tp)\n",
    "    print('True Negatives:', tn)\n",
    "    print('False Positives:', fp)\n",
    "    print('False Negatives:', fn)\n",
    "    print('Total:', dataframe.count(), '\\n')\n",
    "    \n",
    "    print('Accuracy:', acc)\n",
    "    print('Sensitivity:', sen)\n",
    "    print('Specificity:', spe)\n",
    "    print('Precision:', prec)\n",
    "    print('Recall:', rec)\n",
    "    print('F1-score:', f1)\n",
    "    \n",
    "    # Create spark dataframe with results\n",
    "    l = [(acc, sen, spe, prec, rec, f1)]\n",
    "    df = spark.createDataFrame(l, ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'Recall', 'F1'])\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:27:44.955920Z",
     "start_time": "2019-09-04T13:27:42.574281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix and Statistics: \n",
      "\n",
      "+--------------+----+---+\n",
      "|BAD_prediction|   0|  1|\n",
      "+--------------+----+---+\n",
      "|             0|1003| 39|\n",
      "|             1| 162| 77|\n",
      "+--------------+----+---+\n",
      "\n",
      "True Positives: 77\n",
      "True Negatives: 1003\n",
      "False Positives: 39\n",
      "False Negatives: 162\n",
      "Total: 1281 \n",
      "\n",
      "Accuracy: 0.8430913348946136\n",
      "Sensitivity: 0.32217573221757323\n",
      "Specificity: 0.9625719769673704\n",
      "Precision: 0.6637931034482759\n",
      "Recall: 0.32217573221757323\n",
      "F1-score: 0.43380281690140843\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "eval_metrics = evalMetrics('BAD', 'prediction', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:34:57.248735Z",
     "start_time": "2019-09-04T08:34:55.494242Z"
    }
   },
   "source": [
    "For demo purposes, we just training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:51:22.456431Z",
     "start_time": "2019-09-04T10:51:21.579505Z"
    }
   },
   "outputs": [],
   "source": [
    "lrModel.write().overwrite().save('models/logit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment with SAS Model Manager and SAS Workflow Manager\n",
    "\n",
    "Now that the user gets the model, I would like to version its model in an company repository. Then model has to be deployed into Spark environment. \n",
    "In order to do that with SAS Governance enviroment, he needs to register \n",
    "\n",
    "    1) .py with the scoring code (optionally)\n",
    "    \n",
    "    2) Store the model\n",
    "\n",
    "    3) Register the model in SAS Model Manager via REST API call \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:27:48.212799Z",
     "start_time": "2019-09-04T13:27:48.208289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  Code  already exists\n"
     ]
    }
   ],
   "source": [
    "dirName='Code'\n",
    "\n",
    "# Create Output Directory if don't exist\n",
    "\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" created\")\n",
    "else:    \n",
    "    print(\"Directory \" , dirName ,  \" already exists\")\n",
    "    \n",
    "MM_outpath=os.getcwd()+'/'+dirName\n",
    "os.chdir(MM_outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the scoring code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:27:27.573332Z",
     "start_time": "2019-09-24T14:27:27.561332Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:27:30.240611Z",
     "start_time": "2019-09-24T14:27:28.523208Z"
    }
   },
   "outputs": [],
   "source": [
    "dpmodel = LogisticRegressionModel.load(r'/C:/Users/ivnard/Desktop/EDM/ModelManager/MM_2.0/4_spark_integration/spark_native/final/models/logit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:27:42.208476Z",
     "start_time": "2019-09-24T14:27:41.692806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|            features|BAD|\n",
      "+--------------------+---+\n",
      "|(18,[0,1,2,3,6,7,...|  1|\n",
      "|(18,[0,1,2,3,5,6,...|  1|\n",
      "|(18,[0,1,2,3,6,7,...|  1|\n",
      "|(18,[0,1,2,3,6,7,...|  1|\n",
      "|(18,[0,1,2,3,6,8,...|  0|\n",
      "|(18,[0,1,2,3,6,7,...|  1|\n",
      "|[1800.0,48649.0,5...|  1|\n",
      "|(18,[0,1,2,3,6,8,...|  1|\n",
      "|[2000.0,32700.0,4...|  1|\n",
      "|(18,[0,1,2,3,6,8,...|  1|\n",
      "+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('datatoscore.csv', header = True, inferSchema = True)\n",
    "# Assemble features into one vector for the model\n",
    "ignore = ['BAD']\n",
    "assembler = VectorAssembler(inputCols=[x for x in df.columns if x not in ignore],\n",
    "                            outputCol='features')\n",
    "df1 = assembler.transform(df)\n",
    "dataset = df1.select(col('features'),col('BAD'))\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:27:47.151008Z",
     "start_time": "2019-09-24T14:27:46.950418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+--------------------+\n",
      "|BAD|       rawPrediction|prediction|         probability|\n",
      "+---+--------------------+----------+--------------------+\n",
      "|  1|[0.78118344765256...|       0.0|[0.68593511793772...|\n",
      "|  1|[-0.2715116290881...|       1.0|[0.43253603000112...|\n",
      "|  1|[1.14017479299215...|       0.0|[0.75771172976198...|\n",
      "|  1|[1.23685462624181...|       0.0|[0.77501604193595...|\n",
      "|  0|[1.68112168709110...|       0.0|[0.84305300385079...|\n",
      "|  1|[0.70221711230345...|       0.0|[0.66867915107523...|\n",
      "|  1|[-2.5887354093538...|       1.0|[0.06986691828463...|\n",
      "|  1|[0.81381090430379...|       0.0|[0.69292098982905...|\n",
      "|  1|[0.04496481872539...|       0.0|[0.51123931107585...|\n",
      "|  1|[0.30965102367758...|       0.0|[0.57680007768561...|\n",
      "+---+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = dpmodel.transform(dataset)\n",
    "prediction.select('BAD', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:27:56.852064Z",
     "start_time": "2019-09-04T13:27:56.810637Z"
    }
   },
   "outputs": [],
   "source": [
    "split1_udf = udf(lambda value: value[0].item(), FloatType())\n",
    "split2_udf = udf(lambda value: value[1].item(), FloatType())\n",
    "probabilities = prediction.select(split1_udf('probability').alias('P_BAD0'), split2_udf('probability').alias('P_BAD1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:27:58.498949Z",
     "start_time": "2019-09-04T13:27:57.285168Z"
    }
   },
   "outputs": [],
   "source": [
    "df_probabilities=probabilities.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:27:59.559838Z",
     "start_time": "2019-09-04T13:27:59.556830Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_probabilities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:27:59.848627Z",
     "start_time": "2019-09-04T13:27:59.775195Z"
    }
   },
   "outputs": [],
   "source": [
    "df_abt = pd.read_csv(r'C:/Users/ivnard/Desktop/EDM/ModelManager/MM_2.0/4_spark_integration/spark_native/final/datatoscore.csv')\n",
    "target = df_abt['BAD']\n",
    "df_abt.drop(labels=['BAD'], axis=1, inplace = True)\n",
    "df_abt.insert(0, 'BAD', target)\n",
    "df_performances=pd.concat([df_abt, df_probabilities], axis=1)\n",
    "# df_performances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .py with the scoring code (scoring model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:28:13.792242Z",
     "start_time": "2019-09-04T13:28:13.784221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logit_deploy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logit_deploy.py\n",
    "\n",
    "import json, pprint, requests, textwrap\n",
    "\n",
    "data = {'kind': 'pyspark'}\n",
    "r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "r.json()\n",
    "\n",
    "data = {\n",
    "  'code': textwrap.dedent(\"\"\"\n",
    "            \n",
    "            import pandas as pd\n",
    "  \n",
    "            from pyspark.sql.functions import *\n",
    "            from pyspark.sql.types import *\n",
    "            from pyspark.ml.feature import VectorAssembler\n",
    "            from pyspark.ml.classification import LogisticRegressionModel\n",
    "            from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "            # Loads dataset to score and prepare data\n",
    "            \n",
    "            df = spark.read.csv('/tmp/datatoscore.csv', header = True, inferSchema = True)\n",
    "            # Assemble features into one vector for the model\n",
    "            ignore = ['BAD']\n",
    "            assembler = VectorAssembler(inputCols=[x for x in df.columns if x not in ignore],\n",
    "                                        outputCol='features')\n",
    "            df1 = assembler.transform(df)\n",
    "            dataset = df1.select(col('features'),col('BAD'))\n",
    "            \n",
    "            # Loads model\n",
    "            dpmodel = LogisticRegressionModel.load(/tmp/models/logit')\n",
    "            \n",
    "            # Make predictions\n",
    "            prediction = dpmodel.transform(dataset)\n",
    "            \n",
    "            #Generate final scored data\n",
    "            split1_udf = udf(lambda value: value[0].item(), FloatType())\n",
    "            split2_udf = udf(lambda value: value[1].item(), FloatType())\n",
    "            probabilities = prediction./\n",
    "                            select(split1_udf('probability').alias('P_BAD0'), split2_udf('probability').alias('P_BAD1'))\n",
    "            df_probabilities=probabilities.toPandas()\n",
    "            df_abt = pd.read_csv('datatoscore.csv')\n",
    "            target = df_abt['BAD']\n",
    "            df_abt.drop(labels=['BAD'], axis=1, inplace = True)\n",
    "            df_abt.insert(0, 'BAD', target)\n",
    "            df_scored=pd.concat([df_abt, df_probabilities], axis=1)\n",
    "            df_scored.to_csv('tmp/datascored.csv',sep=',',index=False)\n",
    "            \n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers)\n",
    "pprint.pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrModel.write().overwrite().save('logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:28:22.492872Z",
     "start_time": "2019-09-04T13:28:22.024525Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model in SAS Model Manager via REST API call "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip Spark Mlib model score code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:28:30.147633Z",
     "start_time": "2019-09-04T13:28:30.136638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipping C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final/Code\\logit_deploy.py as logit_deploy.py\n"
     ]
    }
   ],
   "source": [
    "sourceDir = MM_outpath\n",
    "modelZipfile = MM_outpath + \".zip\"\n",
    "zipdir(sourceDir, modelZipfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip Spark Mlib Model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:28:41.704041Z",
     "start_time": "2019-09-04T13:28:41.690502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipping C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\models\\logit\\data\\.part-00000-c6a5d4f1-6d9d-4c51-81ea-d33636c3a48b-c000.snappy.parquet.crc as data\\.part-00000-c6a5d4f1-6d9d-4c51-81ea-d33636c3a48b-c000.snappy.parquet.crc\n",
      "zipping C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\models\\logit\\data\\._SUCCESS.crc as data\\._SUCCESS.crc\n",
      "zipping C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\models\\logit\\data\\part-00000-c6a5d4f1-6d9d-4c51-81ea-d33636c3a48b-c000.snappy.parquet as data\\part-00000-c6a5d4f1-6d9d-4c51-81ea-d33636c3a48b-c000.snappy.parquet\n",
      "zipping C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\models\\logit\\data\\_SUCCESS as data\\_SUCCESS\n",
      "zipping C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\models\\logit\\metadata\\.part-00000.crc as metadata\\.part-00000.crc\n",
      "zipping C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\models\\logit\\metadata\\._SUCCESS.crc as metadata\\._SUCCESS.crc\n",
      "zipping C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\models\\logit\\metadata\\part-00000 as metadata\\part-00000\n",
      "zipping C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\models\\logit\\metadata\\_SUCCESS as metadata\\_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "sourceDir = r'C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\models\\logit'\n",
    "modelZipfile = r'C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\SparkLogit'+ \".zip\"\n",
    "zipdir(sourceDir, modelZipfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T14:48:12.314760Z",
     "start_time": "2019-10-02T14:48:11.863711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My token is:\n",
      "eyJhbGciOiJSUzI1NiIsImtpZCI6ImxlZ2FjeS10b2tlbi1rZXkiLCJ0eXAiOiJKV1QifQ.eyJqdGkiOiI3OWYwMDRkYTQzYWU0NWI4OGY0YTI1OTljZGYxMTY1OCIsInN1YiI6IjljOTliOTJkLTQ2ZDEtNDM0MS04MDA1LTNmZjQwYmVhOTM2ZiIsInNjb3BlIjpbIkRhdGFCdWlsZGVycyIsIkFwcGxpY2F0aW9uQWRtaW5pc3RyYXRvcnMiLCJTQVNXb3JrZmxvd0VkaXRvcnMiLCJTQVNXb3JrZmxvd1ZpZXdlcnMiLCJTQVNXb3JrZmxvd0RlZmluaXRpb25BZG1pbmlzdHJhdG9ycyIsInNhcy10ZWNoLWFjY291bnRzIiwic2FzZGVtb191c2VycyIsImNsaWVudHMucmVhZCIsImNsaWVudHMuc2VjcmV0IiwidWFhLnJlc291cmNlIiwib3BlbmlkIiwidWFhLmFkbWluIiwiY2xpZW50cy5hZG1pbiIsInNjaW0ucmVhZCIsImdyb3VwMSIsIlNBU0FkbWluaXN0cmF0b3JzIiwiU0FTV29ya2Zsb3dQcm9jZXNzQWRtaW5pc3RyYXRvcnMiLCJjbGllbnRzLndyaXRlIiwic2NpbS53cml0ZSIsIkNBU0hvc3RBY2NvdW50UmVxdWlyZWQiXSwiY2xpZW50X2lkIjoic2FzLmVjIiwiY2lkIjoic2FzLmVjIiwiYXpwIjoic2FzLmVjIiwiZ3JhbnRfdHlwZSI6InBhc3N3b3JkIiwidXNlcl9pZCI6IjljOTliOTJkLTQ2ZDEtNDM0MS04MDA1LTNmZjQwYmVhOTM2ZiIsImV4dF9pZCI6InVpZD1zYXNkZW1vLGNuPXVzZXJzLGNuPWFjY291bnRzLGRjPXJ1cyxkYz1zYXMsZGM9Y29tIiwib3JpZ2luIjoibGRhcCIsInVzZXJfbmFtZSI6InNhc2RlbW8iLCJlbWFpbCI6InNhc2RlbW9AcnVzLnNhcy5jb20iLCJhdXRoX3RpbWUiOjE1NzAwMjc2ODMsInJldl9zaWciOiI0MjQ3ZTdkOSIsImlhdCI6MTU3MDAyNzY4MywiZXhwIjoxNTcwMDcwODgzLCJpc3MiOiJodHRwOi8vbG9jYWxob3N0L1NBU0xvZ29uL29hdXRoL3Rva2VuIiwiemlkIjoidWFhIiwiYXVkIjpbInNjaW0iLCJjbGllbnRzIiwic2FzLioiLCJ1YWEiLCJvcGVuaWQiLCJzYXMuZWMiXX0.NOEb8z6MWNLjl1nsRhtv84N9hNbR5P3V8U26b75W35c5aRqIdwB8OjK87iJBOGBay78N7XFu4JSZLT9VyINzgfHmvNXVNtsFd3CRcUnMC45Nn1zGdluOv2RbkfFMZXECWATeepd8-oXe6qWkeUur_J4TwjK3cOYfj7gAwjSBApI\n"
     ]
    }
   ],
   "source": [
    "protocol = 'http'\n",
    "server = 'rusretailviya.rus.sas.com'\n",
    "authUri = '/SASLogon/oauth/token'\n",
    "user = 'sasdemo'\n",
    "password = 'Orion123'\n",
    "mmService = protocol + '://' + server\n",
    "\n",
    "headers = {\n",
    "   'Accept': 'application/json',\n",
    "   'Content-Type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "\n",
    "payload = 'grant_type=password&username=' + user + '&password=' + password\n",
    "authReturn = req.post(mmService + authUri , auth=('sas.ec', ''), data=payload, headers=headers);      \n",
    "authJson = json.loads(authReturn.content.decode('utf-8'))\n",
    "myToken = authJson['access_token']\n",
    "print(\"My token is:\\n\" + myToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Model Manager repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T14:48:34.514283Z",
     "start_time": "2019-10-02T14:48:31.446162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository name  = MMSPARK5\n",
      "Repository ID    = 7b5edb6c-ade4-4767-9bef-604fee81f26b\n",
      "Parent folder ID = f1d9bf8c-099b-4b04-bf3d-912157da6bb0\n"
     ]
    }
   ],
   "source": [
    "payload = {}\n",
    "headers = {    \n",
    "    'content-type': 'application/vnd.sas.models.repository+json',\n",
    "    'Authorization':'bearer %s' % myToken\n",
    "    } \n",
    "payload[\"name\"] = \"MMSPARK%d\" % randid\n",
    "print(\"Repository name  = \" + payload[\"name\"])\n",
    "\n",
    "reply = req.post(mmService + \"/modelRepository/repositories\", data=json.dumps(payload), headers=headers)\n",
    "restRep = json.loads(reply.content.decode('utf8'))\n",
    "\n",
    "reposID = restRep['id'] \n",
    "print(\"Repository ID    = \" + reposID)\n",
    "\n",
    "parentFolderID = restRep['folderId']\n",
    "print(\"Parent folder ID = \" + parentFolderID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Model Manager Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T14:48:35.820159Z",
     "start_time": "2019-10-02T14:48:35.664712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name = MMSPARKFolder5\n",
      "Folder ID = ed28a961-7ff3-4ef9-8990-3288b6b9c000\n"
     ]
    }
   ],
   "source": [
    "headersFolder={\n",
    "    \"content-type\": \"application/json\",\n",
    "    'Authorization':'bearer %s' % myToken\n",
    "}\n",
    "\n",
    "parentURI = '/folders/folders/' + parentFolderID\n",
    "folderName = \"MMSPARKFolder%d\" % randid\n",
    "print(\"Folder name = \" + folderName)\n",
    "\n",
    "newFolder = {\"name\":folderName}\n",
    "reply = req.post(mmService+ '/folders/folders?parentFolderUri=' + parentURI, data=json.dumps(newFolder), headers=headersFolder);\n",
    "myFolder = json.loads(reply.content.decode('utf-8'))\n",
    "\n",
    "folderID = myFolder['id']\n",
    "print(\"Folder ID = \" + folderID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a New Model Manager Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T14:48:38.703601Z",
     "start_time": "2019-10-02T14:48:37.364698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name = MMSPARKClassification5\n",
      "Project ID = 9038c453-464c-45ac-a37c-aec915b9c66c\n"
     ]
    }
   ],
   "source": [
    "headersProj={\n",
    "    'content-type': 'application/vnd.sas.models.project+json',\n",
    "    'Authorization':'bearer %s' % myToken\n",
    "}\n",
    "\n",
    "projName=\"MMSPARKClassification%d\" % randid\n",
    "print(\"Project name = \" + projName)\n",
    "\n",
    "newProj = {\n",
    "    \"name\": projName,\n",
    "    \"repositoryId\": reposID,\n",
    "    \"folderId\": folderID,\n",
    "    \"function\": \"Classification\",\n",
    "    \"targetLevel\": \"Binary\"\n",
    "}\n",
    "\n",
    "reply = req.post(mmService + '/modelRepository/projects', data=json.dumps(newProj), headers = headersProj)\n",
    "myProj = reply.json()\n",
    "\n",
    "projectID = myProj['id']\n",
    "print(\"Project ID = \" + projectID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the score code to SAS Model Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T14:48:52.648427Z",
     "start_time": "2019-10-02T14:48:49.975142Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model name = Spark MLib Logistic Regression5\n",
      "Custom model ID = 07a16231-9e5f-4b36-a549-392979ac3504\n"
     ]
    }
   ],
   "source": [
    "#Load zip files\n",
    "mfile = open(r'C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\Code.zip', 'rb')\n",
    "\n",
    "modelName =\"Spark MLib Logistic Regression\" + str(randid)\n",
    "print(\"Custom model name = \" + modelName)\n",
    "\n",
    "headersModel = {\n",
    "    'Authorization':'bearer '+ myToken\n",
    "}\n",
    "\n",
    "files = {'files': (modelName+\".zip\", mfile, 'multipart/form-data')}\n",
    "\n",
    "reply = req.post(mmService + \"/modelRepository/models?name=\" + modelName + \"&type=ZIP&projectId=\" + projectID +\"&versionOption=Latest\", \n",
    "                        files=files, headers=headersModel)\n",
    "myModel = json.loads(reply.content.decode('utf-8'))\n",
    "mfile.close()\n",
    "\n",
    "modelID = myModel['items'][0]['id']\n",
    "print(\"Custom model ID = \" + modelID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the Spark Model files to SAS Model Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T14:48:54.957337Z",
     "start_time": "2019-10-02T14:48:54.558623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark model files name = SparkLogit.zip\n",
      "Spark model files ID = 12a35033-ca29-4d39-b961-cb810185d107\n"
     ]
    }
   ],
   "source": [
    "#Load zip files\n",
    "mfile = open(r'C:\\Users\\ivnard\\Desktop\\EDM\\ModelManager\\MM_2.0\\4_spark_integration\\spark_native\\final\\SparkLogit.zip', 'rb')\n",
    "\n",
    "mdfilesname = \"SparkLogit.zip\"\n",
    "print(\"Spark model files name = \" + mdfilesname)\n",
    "\n",
    "headersModel = {\n",
    "    'Authorization':'bearer '+ myToken\n",
    "}\n",
    "\n",
    "files = {'files': (mdfilesname, mfile, 'multipart/form-data')}\n",
    "\n",
    "# reply = req.post(mmService + \"/modelRepository/models?name=\" + modelName + \"&type=ZIP&projectId=\" + projectID +\"&versionOption=Latest\", \n",
    "#                         files=files, headers=headersModel)\n",
    "\n",
    "reply = req.post(mmService + \"/modelRepository/models/\" + modelID + \"/contents\", \n",
    "                 files=files, headers=headersModel)\n",
    "\n",
    "newModel = json.loads(reply.content.decode('utf-8'))\n",
    "mfile.close()\n",
    "\n",
    "modelID = newModel['items'][0]['id']\n",
    "print(\"Spark model files ID = \" + modelID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python codes for SAS Workflow Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .py for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile logit_retrain.py\n",
    "\n",
    "import json, pprint, requests, textwrap\n",
    "\n",
    "data = {'kind': 'pyspark'}\n",
    "r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "r.json()\n",
    "\n",
    "data = {\n",
    "  'code': textwrap.dedent(\"\"\"\n",
    "            \n",
    "            import pandas as pd\n",
    "            from pyspark.sql.functions import *\n",
    "            from pyspark.sql.types import *\n",
    "            from pyspark.ml.feature import VectorAssembler\n",
    "            from pyspark.ml.classification import LogisticRegressionModel\n",
    "            \n",
    "\n",
    "            # Loads Model\n",
    "            from pyspark.ml.classification import LogisticRegressionModel\n",
    "            dpmodel = LogisticRegressionModel.load(\"/tmp/models/logit\")\n",
    "            dpmodel\n",
    "            \n",
    "            # Assemble features into one vector for the model\n",
    "            from pyspark.sql.functions import *\n",
    "            from pyspark.sql.types import *\n",
    "            from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "            df = spark.read.csv('/tmp/datatoscore.csv', header = True, inferSchema = True)\n",
    "            # Assemble features into one vector for the model\n",
    "            ignore = ['BAD']\n",
    "            assembler = VectorAssembler(inputCols=[x for x in df.columns if x not in ignore],\n",
    "                                        outputCol='features')\n",
    "            df1 = assembler.transform(df)\n",
    "            ds = df1.select(col('features'),col('BAD'))\n",
    "            \n",
    "            #Score data\n",
    "            paramMap={dpmodel.threshold: 0.5}\n",
    "            prediction = dpmodel.transform(ds, paramMap)\n",
    "            \n",
    "            #Store Performance table\n",
    "            split1_udf = udf(lambda value: value[0].item(), FloatType())\n",
    "            split2_udf = udf(lambda value: value[1].item(), FloatType())\n",
    "            probabilities = prediction.select(split1_udf('probability').alias('P_BAD0'), split2_udf('probability').alias('P_BAD1'))\n",
    "            df_probabilities=probabilities.toPandas()\n",
    "            df_abt = pd.read_csv('datatoscore.csv')\n",
    "            target = df_abt['BAD']\n",
    "            df_abt.drop(labels=['BAD'], axis=1, inplace = True)\n",
    "            df_abt.insert(0, 'BAD', target)\n",
    "            df_performances=pd.concat([df_abt, df_probabilities], axis=1)\n",
    "            df_performances.to_csv('perfomance_table.csv',sep=',',header=True)\n",
    "            \n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers)\n",
    "pprint.pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .py for retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile logit_retrain.py\n",
    "\n",
    "import json, pprint, requests, textwrap\n",
    "\n",
    "data = {'kind': 'pyspark'}\n",
    "r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "r.json()\n",
    "\n",
    "data = {\n",
    "  'code': textwrap.dedent(\"\"\"\n",
    "            \n",
    "            import pandas as pd\n",
    "            from pyspark.sql.functions import *\n",
    "            from pyspark.sql.types import *\n",
    "            from pyspark.ml.feature import VectorAssembler\n",
    "            from pyspark.ml.classification import LogisticRegressionModel\n",
    "            \n",
    "\n",
    "            # Loads datatest\n",
    "            df = spark.read.csv('/tmp/datatoscore.csv', header = True, inferSchema = True)\n",
    "            \n",
    "            # Assemble features into one vector for the model\n",
    "            ignore = ['BAD']\n",
    "            assembler = VectorAssembler(inputCols=[x for x in df.columns if x not in ignore],\n",
    "                                        outputCol='features')\n",
    "            df1 = assembler.transform(df)\n",
    "            dataset = df1.select(col('features'),col('BAD'))\n",
    "            \n",
    "            #Train and test split\n",
    "            train, test = dataset.randomSplit([0.8, 0.2], seed=83)\n",
    "            \n",
    "            #Train a Logistic model\n",
    "            lr = LogisticRegression(labelCol = 'BAD', maxIter=10)\n",
    "            lrModel = lr.fit(train)\n",
    "            \n",
    "            #Store new model\n",
    "            lrModel.write().overwrite().save('models/logit')      \n",
    "            \n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers)\n",
    "pprint.pprint(r.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280.741px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
